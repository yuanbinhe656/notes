1. 层

    1. convolution 卷积

        1. 

    2. Pooling 池化

        1. 池化（Pooling）是深度学习中用于减少特征图尺寸和提取关键信息的一种操作。在卷积神经网络（CNN）中经常会用到池化层，其作用是降低每个特征映射的空间尺寸，同时保留重要信息。

            常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）：

            1. **最大池化（Max Pooling）：** 在给定区域内（通常是2x2或3x3的窗口）选取每个特征图的最大值作为池化后的值。这有助于保留最显著的特征，同时减少计算量和参数数量。
            2. **平均池化（Average Pooling）：** 与最大池化类似，但是它计算选定区域内的平均值作为池化后的值。这种方式也可以降低特征图的维度，但不像最大池化那样强调最显著的特征。

            池化操作的一些优势包括：

            - **降低计算成本：** 通过降低特征图的维度，池化可以减少后续层的计算负担。
            - **减少过拟合：** 池化可以减少特征图的尺寸，有助于减少模型过度拟合训练数据的可能性。
            - **提取显著特征：** 最大池化有助于提取最显著的特征，这些特征对于识别图像中的重要模式非常有用。

            池化层通常与卷积层交替使用，构成卷积神经网络的主要部分。通过卷积提取特征后，池化操作有助于降低维度并保留最重要的特征，为神经网络的后续层提供更加精炼的输入。

        2. 

    3. Softmax 输出

        1. Softmax 函数是一个常用的激活函数，特别在分类问题中广泛使用。它通常被用于输出层，用于将一个K维的向量（K个实数值）映射为一个概率分布，其中每个元素的取值范围在 0 到 1 之间，并且所有元素的和为 1。

            Softmax 函数的数学表达式如下：

            \[ \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \]

            其中，\(\mathbf{z}\) 是一个长度为 K 的实数向量，\(\text{softmax}(\mathbf{z})_i\) 表示 softmax 函数的输出中的第 i 个元素。

            Softmax 函数的作用是将输入的实数向量转换为概率分布，其中每个元素对应一个类别的概率。在神经网络的多分类问题中，通常用 Softmax 函数来计算每个类别的概率分布，然后根据概率最大的类别来进行预测。

            Softmax 函数的特性包括：

            1. **将输出转化为概率分布：** Softmax 可以将神经网络输出的原始分数转换为概率分布，便于理解和解释模型的输出。

            2. **对输入进行归一化：** 每个类别的输出值被归一化为 [0, 1] 范围内，并且所有类别的输出值之和为 1，表示概率总和。

            3. **对大数值敏感：** Softmax 函数会放大最大的输入值对应的输出概率，这意味着它对于输入值的相对大小比较敏感。

            Softmax 在神经网络中被广泛应用，尤其是在多分类问题中用于输出层，以获得每个类别的概率分布，从而进行分类预测。

    4. Other 拼接层

2. 减少代码冗余

    1. 函数
    2. 类   inception 
        1. 将一个块封装成一个类，这个类中的层种类和数量相同

3. inception 